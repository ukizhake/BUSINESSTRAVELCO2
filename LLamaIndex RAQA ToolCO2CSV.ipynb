{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on terminology:\n",
    "\n",
    "You'll notice that there are quite a few similarities between LangChain and LlamaIndex. LlamaIndex can largely be thought of as an extension to LangChain, in some ways - but they moved some of the language around. Let's spend a few moments disambiguating the language.\n",
    "\n",
    "- `QueryEngine` -> `RetrievalQA`:\n",
    "  -  `QueryEngine` is just LlamaIndex's way of indicating something is an LLM \"chain\" on top of a retrieval system\n",
    "- `OpenAIAgent` vs. `ZeroShotAgent`:\n",
    "  - The two agents have the same fundamental pattern: Decide which of a list of tools to use to answer a user's query.\n",
    "  - `OpenAIAgent` (LlamaIndex's primary agent) does not need to rely on an agent excecutor due to the fact that it is leveraging OpenAI's [functional api](https://openai.com/blog/function-calling-and-other-api-updates) which allows the agent to interface \"directly\" with the tools instead of operating through an intermediary application process.\n",
    "\n",
    "There is, however, a much large terminological difference when it comes to discussing data.\n",
    "\n",
    "##### Nodes vs. Documents\n",
    "\n",
    "As you're aware of from the previous weeks assignments, there's an idea of `documents` in NLP which refers to text objects that exist within a corpus of documents.\n",
    "\n",
    "LlamaIndex takes this a step further and reclassifies `documents` as `nodes`. Confusingly, it refers to the `Source Document` as simply `Documents`.\n",
    "\n",
    "The `Document` -> `node` structure is, almost exactly, equivalent to the `Source Document` -> `Document` structure found in LangChain - but the new terminology comes with some clarity about different structure-indices. \n",
    "\n",
    "We won't be leveraging those structured indicies today, but we will be leveraging a \"benefit\" of the `node` structure that exists as a default in LlamaIndex, which is the ability to quickly filter nodes based on their metadata.\n",
    "\n",
    "![image](https://i.imgur.com/B1QDjs5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a more robust RAQA system using LlamaIndex\n",
    "\n",
    "We'll be putting together a system for querying both qualitative and quantitative data using LlamaIndex. \n",
    "\n",
    "To stick to a theme, we'll continue to use BarbenHeimer data as our base - but this can, and should, be extended to other topics/domains.\n",
    "\n",
    "# Build 🏗️\n",
    "There are 3 main tasks in this notebook:\n",
    "\n",
    "- Create a Qualitative VectorStore query engine\n",
    "- Create a quantitative NLtoSQL query engine\n",
    "- Combine the two using LlamaIndex's OpenAI agent framework.\n",
    "\n",
    "# Ship 🚢\n",
    "Create an host a Gradio or Chainlit application to serve your project on Hugging Face spaces.\n",
    "\n",
    "# Share 🚀\n",
    "Make a social media post about your final application and tag @AIMakerspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOILERPLATE\n",
    "\n",
    "This is only relevant when running the code in a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary Dependencies and Context Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies and OpenAI API key setting\n",
    "\n",
    "First of all, we'll need our primary libraries - and to set up our OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q openai==0.27.8 llama-index==0.8.40 nltk==3.8.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.8.40'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index\n",
    "\n",
    "llama_index.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (2.0.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: requests==2.31.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from arxiv) (2.31.0)\n",
      "Requirement already satisfied: sgmllib3k in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:arxiv.arxiv:Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=CDP+Questionnaire+with+regards+transport&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=CDP+Questionnaire+with+regards+transport&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "INFO:arxiv.arxiv:Got first page: 50 of 1683690 total results\n",
      "Got first page: 50 of 1683690 total results\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WikipediaLoader, CSVLoader, ArxivLoader\n",
    "from aimakerspace.text_utils import TextFileLoader,CharacterTextSplitter\n",
    "\n",
    "sec_wikipedia_docs = ArxivLoader(\n",
    "    query=\"CDP Questionnaire with regards transport\", \n",
    "    load_max_docs= 5,# YOUR CODE HERE, \n",
    "    doc_content_chars_max=1_000_000### YOUR CODE HERE\n",
    "    ).load()\n",
    "text_loader=TextFileLoader(\"./data/BusinessTravelReport.txt\")\n",
    "business_travel_documents = text_loader.load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "import openai\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_API_KEY\"] = getpass.getpass(\"WandB API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (3.1.36)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (1.31.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_API_KEY\"] = getpass.getpass(\"WandB API Key: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (3.1.36)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (1.31.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement wandb_callback (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for wandb_callback\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LlamaIndex events to W&B at https://wandb.ai/sumush/llamaindex-demo-v1/runs/dcqo5nko\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbCallbackHandler` is currently in beta.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `llamaindex`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index import set_global_handler\n",
    "\n",
    "set_global_handler(\"wandb\", run_args={\"project\": \"llamaindex-demo-v1\"})\n",
    "wandb_callback = llama_index.global_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Setting\n",
    "\n",
    "Now, LlamaIndex has the ability to set `ServiceContext`. You can think of this as a config file of sorts. The basic idea here is that we use this to establish some core properties and then can pass it to various services. \n",
    "\n",
    "While we could set this up as a global context, we're going to leave it as `ServiceContext` so we can see where it's applied.\n",
    "\n",
    "We'll set a few significant contexts:\n",
    "\n",
    "- `chunk_size` - this is what it says on the tin\n",
    "- `llm` - this is where we can set what model we wish to use as our primary LLM when we're making `QueryEngine`s and more\n",
    "- `embed_model` - this will help us keep our embedding model consistent across use cases\n",
    "\n",
    "\n",
    "We'll also create some resources we're going to keep consistent across all of our indices today.\n",
    "\n",
    "- `text_splitter` - This is what we'll use to split our text, feel free to experiment here\n",
    "- `SimpleNodeParser` - This is what will work in tandem with the `text_splitter` to parse our full sized documents into nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.node_parser.simple import SimpleNodeParser\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding()### YOUR CODE HERE\n",
    "chunk_size = 500### YOUR CODE HERE\n",
    "llm = OpenAI(\n",
    "    temperature=0,### YOUR CODE HERE\n",
    "    model=\"gpt-3.5-turbo-0613\",### YOUR CODE HERE\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,### YOUR CODE HERE\n",
    "    chunk_size=chunk_size,### YOUR CODE HERE\n",
    "    embed_model=embed_model### YOUR CODE HERE\n",
    ")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=chunk_size### YOUR CODE HERE\n",
    ")\n",
    "\n",
    "node_parser = SimpleNodeParser(\n",
    "    text_splitter=text_splitter### YOUR CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Wikipedia Retrieval Tool\n",
    "\n",
    "Now we can get to work creating our semantic `QueryEngine`!\n",
    "\n",
    "We'll follow a similar pattern as we did with LangChain here - and the first step (as always) is to get dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q tiktoken==0.4.0 sentence-transformers==2.2.2 pydantic==1.10.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPTIndex\n",
    "\n",
    "We'll be using [GPTIndex](https://gpt-index.readthedocs.io/en/v0.6.2/reference/indices/vector_store.html) as our `VectorStore` today!\n",
    "\n",
    "It works in a similar fashion to tools like Pinecone, Weaveate, and more - but it's locally hosted and will serve our purposes fine. \n",
    "\n",
    "Also the `GPTIndex` is integrated with WandB for index versioning.\n",
    "\n",
    "You'll also notice the return of `OpenAIEmbedding()`, which is the embeddings model we'll be leveraging. Of course, this is using the `ada` model under the hood - and already comes equipped with in-memory caching.\n",
    "\n",
    "You'll notice we can pass our `service_context` into our `VectorStoreIndex`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents([], service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (3.1.36)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (1.31.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement wandb_callback (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for wandb_callback\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q wikipedia\n",
    "%pip install wandb\n",
    "%pip install wandb_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially the same as the LangChain example - we're just going to be pulling information straight from Wikipedia using the built in `WikipediaReader`.\n",
    "\n",
    "Setting `auto_suggest=False` ensures we run into fewer auto-correct based errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "\n",
    "cdp_list = [\n",
    "    \"Carbon Disclosure Project\", \n",
    "    \"Carbon Disclosure Project\"\n",
    "]\n",
    "\n",
    "wiki_docs = WikipediaReader().load_data(\n",
    "    pages=cdp_list,\n",
    "    auto_suggest=False\n",
    "    ### YOUR CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Construction\n",
    "\n",
    "Now we will loop through our documents and metadata and construct nodes (associated with particular metadata for easy filtration later).\n",
    "\n",
    "We're using the `node_parser` we created at the top of the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cdp_doc, wiki_doc in zip(cdp_list, wiki_docs):\n",
    "    nodes = node_parser.get_nodes_from_documents([wiki_doc])\n",
    "    for node in nodes:\n",
    "        node.metadata = {\"title\" : cdp_doc}\n",
    "    index.insert_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement wandb_callback (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for wandb_callback\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week3-thurs/Week 3/Thursday/wandb/run-20231202_161558-dcqo5nko/files/storage)... Done. 0.0s\n"
     ]
    }
   ],
   "source": [
    "wandb_callback.persist_index(index, index_name=\"wiki-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement wandb_callback (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for wandb_callback\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Failed to log trace tree to W&B: list index out of range\n"
     ]
    }
   ],
   "source": [
    "from llama_index import load_index_from_storage\n",
    "\n",
    "storage_context = wandb_callback.load_storage_context(\n",
    "    artifact_url=\"sumush/llamaindex-demo-v1/wiki-index:v33\" ### YOUR ARTIFACT URL HERE\n",
    ")\n",
    "\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StorageContext(docstore=<llama_index.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x2a5848290>, index_store=<llama_index.storage.index_store.simple_index_store.SimpleIndexStore object at 0x2a5b0fc90>, vector_store=<llama_index.vector_stores.simple.SimpleVectorStore object at 0x2a5b1fc90>, graph_store=<llama_index.graph_stores.simple.SimpleGraphStore object at 0x2a56f7a10>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb_callback.load_storage_context(artifact_url=\"sumush/llamaindex-demo-v1/wiki-index:v33\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto Retriever Functional Tool\n",
    "\n",
    "This tool will leverage OpenAI's functional endpoint to select the correct metadata filter and query the filtered index - only looking at nodes with the desired metadata.\n",
    "\n",
    "A simplified diagram: ![image](https://i.imgur.com/AICDPav.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create our `VectoreStoreInfo` object which will hold all the relevant metadata we need for each component (in this case title metadata).\n",
    "\n",
    "Notice that you need to include it in a text list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import FunctionTool\n",
    "from llama_index.vector_stores.types import (\n",
    "    VectorStoreInfo,\n",
    "    MetadataInfo,\n",
    "    ExactMatchFilter,\n",
    "    MetadataFilters,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "from typing import List, Tuple, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "top_k = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our base PyDantic object that we can use to ensure compatability with our application layer. This verifies that the response from the OpenAI endpoint conforms to this schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRetrieveModel(BaseModel):\n",
    "    query: str = Field(..., description=\"natural language query string\")\n",
    "    filter_key_list: List[str] = Field(\n",
    "        ..., description=\"List of metadata filter field names\"\n",
    "    )\n",
    "    filter_value_list: List[str] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"List of metadata filter field values (corresponding to names specified in filter_key_list)\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build our function that we will use to query the functional endpoint.\n",
    "\n",
    ">The `docstring` is important to the functionality of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_retrieve_fn(\n",
    "    query: str, filter_key_list: List[str], filter_value_list: List[str]\n",
    "):\n",
    "    \"\"\"Auto retrieval function.\n",
    "\n",
    "    Performs auto-retrieval from a vector database, and then applies a set of filters.\n",
    "\n",
    "    \"\"\"\n",
    "    query = query or \"Query\"\n",
    "\n",
    "    exact_match_filters = [\n",
    "        ExactMatchFilter(key=k, value=v)\n",
    "        for k, v in zip(filter_key_list, filter_value_list)\n",
    "    ]\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index, filters=MetadataFilters(filters=exact_match_filters), top_k=top_k\n",
    "    )\n",
    "    query_engine = RetrieverQueryEngine.from_args(retriever, service_context=service_context)\n",
    "\n",
    "    response = query_engine.query(query)\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to wrap our system in a tool in order to integrate it into the larger application.\n",
    "\n",
    "Source Code Here:\n",
    "- [`FunctionTool`](https://github.com/jerryjliu/llama_index/blob/d24767b0812ac56104497d8f59095eccbe9f2b08/llama_index/tools/function_tool.py#L21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable \n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"semantic information about carbon disclosure\",\n",
    "    metadata_info=[MetadataInfo(\n",
    "        name=\"title\",\n",
    "        type=\"str\",\n",
    "        description=\"title of the emissions reporting methods, one of [Carbon Disclosure Project air, Carbon Disclosure Project rail]\",\n",
    "        # to_openai_function=\n",
    "    )]\n",
    ")\n",
    "description = f\"\"\"\\\n",
    "Use this tool to look up semantic information about films.\n",
    "The vector database schema is given below:\n",
    "{vector_store_info.json()}\n",
    "\"\"\"\n",
    "\n",
    "auto_retrieve_tool = FunctionTool.from_defaults(\n",
    "    fn=auto_retrieve_fn,### YOUR CODE HERE\n",
    "    name=\"semantic-cdp-info\",### YOUR CODE HERE\n",
    "    description=description,### YOUR CODE HERE\n",
    "    fn_schema=AutoRetrieveModel,### YOUR CODE HERE\n",
    "    # tool_metadata=vector_store_info.metadata_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is attach the tool to an OpenAIAgent and let it rip!\n",
    "\n",
    "Source Code Here:\n",
    "- [`OpenAIAgent`](https://github.com/jerryjliu/llama_index/blob/d24767b0812ac56104497d8f59095eccbe9f2b08/llama_index/agent/openai_agent.py#L361)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    tools=[\n",
    "        auto_retrieve_tool### YOUR CODE HERE\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.tools.function_tool.FunctionTool object at 0x2a5bf9890>]\n",
      "ToolMetadata(description='Use this tool to look up semantic information about films.\\nThe vector database schema is given below:\\n{\"metadata_info\": [{\"name\": \"title\", \"type\": \"str\", \"description\": \"title of the emissions reporting methods, one of [Carbon Disclosure Project air, Carbon Disclosure Project rail]\"}], \"content_info\": \"semantic information about carbon disclosure\"}\\n', name='semantic-cdp-info', fn_schema=<class '__main__.AutoRetrieveModel'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response=\"I'm sorry, but I couldn't find any specific information about different business travel carbon emissions in the Carbon Disclosure Project database.\", sources=[ToolOutput(content='Empty Response', tool_name='semantic-cdp-info', raw_input={'args': (), 'kwargs': {'query': 'business travel carbon emissions', 'filter_key_list': ['title'], 'filter_value_list': ['Carbon Disclosure Project air']}}, raw_output='Empty Response')], source_nodes=[])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat(\"what are the different business travel carbon emissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.tools.function_tool.FunctionTool object at 0x2a5bf9890>]\n",
      "ToolMetadata(description='Use this tool to look up semantic information about films.\\nThe vector database schema is given below:\\n{\"metadata_info\": [{\"name\": \"title\", \"type\": \"str\", \"description\": \"title of the emissions reporting methods, one of [Carbon Disclosure Project air, Carbon Disclosure Project rail]\"}], \"content_info\": \"semantic information about carbon disclosure\"}\\n', name='semantic-cdp-info', fn_schema=<class '__main__.AutoRetrieveModel'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Carbon Disclosure Project (CDP) is an international non-profit organization that works with companies, cities, states, and regions to help them disclose their environmental impact. CDP collects and analyzes data on carbon emissions, water usage, and deforestation from thousands of organizations worldwide. This data is then made available to investors, corporations, and policymakers to inform decision-making and drive action towards a more sustainable economy. CDP's mission is to encourage transparency and accountability in environmental reporting, and to promote the adoption of sustainable practices to address climate change and other environmental challenges.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Tell me briefly about the Carbon Disclosure Project  \")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business travel air SQL Tool\n",
    "\n",
    "We'll walk through the steps of creating a natural language to SQL system in the following section.\n",
    "\n",
    "> NOTICE: This does not have parsing on the inputs or intermediary calls to ensure that users are using safe SQL queries. Use this with caution in a production environment without adding specific guardrails from either side of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U sqlalchemy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few steps should be largely straightforward, we'll want to:\n",
    "\n",
    "1. Read in our `.csv` files into `pd.DataFrame` objects\n",
    "2. Create an in-memory `sqlite` powered `sqlalchemy` engine\n",
    "3. Cast our `pd.DataFrame` objects to the SQL engine\n",
    "4. Create an `SQLDatabase` object through LlamaIndex\n",
    "5. Use that to create a `QueryEngineTool` that we can interact with through the `NLSQLTableQueryEngine`!\n",
    "\n",
    "If you get stuck, please consult the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read `.csv` Into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cdp_business_travel_air_df = pd.read_csv(\"./data/BusinessTravelAir.csv\")\n",
    "cdp_business_travel_rail_df = pd.read_csv(\"./data/BusinessTravelRail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Table 3.  Air,,,,,,\\r\\n ID, Description, Length,Miles\"</th>\n",
       "      <th>CO2 Emissions</th>\n",
       "      <th>CH4 Emissions</th>\n",
       "      <th>N2O Emissions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JD-001</th>\n",
       "      <th>John Doe 1</th>\n",
       "      <th>Medium Haul (&gt;= 300 miles, &lt; 2300 miles)</th>\n",
       "      <td>1,000</td>\n",
       "      <td>229</td>\n",
       "      <td>10.4</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JD-002</th>\n",
       "      <th>Mark</th>\n",
       "      <th>Short Haul (&lt; 300 miles)</th>\n",
       "      <td>2,200</td>\n",
       "      <td>455</td>\n",
       "      <td>14.1</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JD-003</th>\n",
       "      <th>Chris</th>\n",
       "      <th>Long Haul (&gt;= 2300 miles)</th>\n",
       "      <td>4,799</td>\n",
       "      <td>782</td>\n",
       "      <td>2.9</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JD-004</th>\n",
       "      <th>Brian</th>\n",
       "      <th>Medium Haul (&gt;= 300 miles, &lt; 2300 miles)</th>\n",
       "      <td>2,000</td>\n",
       "      <td>258</td>\n",
       "      <td>1.2</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           Table 3.  Air,,,,,,\\r\\n ID, Description, Length,Miles\"  \\\n",
       "JD-001 John Doe 1 Medium Haul (>= 300 miles, < 2300 miles)                                              1,000       \n",
       "JD-002 Mark       Short Haul (< 300 miles)                                                              2,200       \n",
       "JD-003 Chris      Long Haul (>= 2300 miles)                                                             4,799       \n",
       "JD-004 Brian      Medium Haul (>= 300 miles, < 2300 miles)                                              2,000       \n",
       "\n",
       "                                                            CO2 Emissions  \\\n",
       "JD-001 John Doe 1 Medium Haul (>= 300 miles, < 2300 miles)            229   \n",
       "JD-002 Mark       Short Haul (< 300 miles)                            455   \n",
       "JD-003 Chris      Long Haul (>= 2300 miles)                           782   \n",
       "JD-004 Brian      Medium Haul (>= 300 miles, < 2300 miles)            258   \n",
       "\n",
       "                                                            CH4 Emissions  \\\n",
       "JD-001 John Doe 1 Medium Haul (>= 300 miles, < 2300 miles)           10.4   \n",
       "JD-002 Mark       Short Haul (< 300 miles)                           14.1   \n",
       "JD-003 Chris      Long Haul (>= 2300 miles)                           2.9   \n",
       "JD-004 Brian      Medium Haul (>= 300 miles, < 2300 miles)            1.2   \n",
       "\n",
       "                                                            N2O Emissions  \n",
       "JD-001 John Doe 1 Medium Haul (>= 300 miles, < 2300 miles)            8.5  \n",
       "JD-002 Mark       Short Haul (< 300 miles)                           14.5  \n",
       "JD-003 Chris      Long Haul (>= 2300 miles)                          25.0  \n",
       "JD-004 Brian      Medium Haul (>= 300 miles, < 2300 miles)            8.2  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdp_business_travel_air_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create SQLAlchemy engine with SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"sqlite+pysqlite:///:memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert `pd.DataFrame` to SQL tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdp_business_travel_air_df.to_sql(\n",
    "    \"cdp_business_travel_air\",\n",
    "    engine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdp_business_travel_rail_df.to_sql(\n",
    "    \"cdp_business_travel_rail\",\n",
    "    engine\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct a `SQLDatabase` index\n",
    "\n",
    "Source Code Here:\n",
    "- [`SQLDatabase`](https://github.com/jerryjliu/llama_index/blob/d24767b0812ac56104497d8f59095eccbe9f2b08/llama_index/langchain_helpers/sql_wrapper.py#L9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SQLDatabase\n",
    "\n",
    "sql_database = SQLDatabase(\n",
    "    engine=engine,\n",
    "    include_tables=[\n",
    "        \"cdp_business_travel_air\",### YOUR CODE HERE\n",
    "        \"cdp_business_travel_rail\",### YOUR CODE HER\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the NLSQLTableQueryEngine interface for all added SQL tables\n",
    "\n",
    "Source Code Here:\n",
    "- [`NLSQLTableQueryEngine`](https://github.com/jerryjliu/llama_index/blob/d24767b0812ac56104497d8f59095eccbe9f2b08/llama_index/indices/struct_store/sql_query.py#L75C1-L75C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.struct_store.sql_query import NLSQLTableQueryEngine\n",
    "\n",
    "sql_query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database,### YOUR CODE HERE\n",
    "    tables=[\n",
    "        \"cdp_business_travel_air\",### YOUR CODE HERE,\n",
    "        \"cdp_business_travel_rail\",### YOUR CODE HER\n",
    "    ],### YOUR CODE HERE, \n",
    "    service_context=service_context,### YOUR CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap It All Up in a `QueryEngineTool`\n",
    "\n",
    "You'll want to ensure you have a descriptive...description. \n",
    "\n",
    "An example is provided here:\n",
    "\n",
    "```\n",
    "\"Useful for translating a natural language query into a SQL query over a table containing: \"\n",
    "\"barbie, containing information related to reviews of the Barbie movie\"\n",
    "\"oppenheimer, containing information related to reviews of the Oppenheimer movie\"\n",
    "```\n",
    "\n",
    "Sorce Code Here: \n",
    "\n",
    "- [`QueryEngineTool`](https://github.com/jerryjliu/llama_index/blob/d24767b0812ac56104497d8f59095eccbe9f2b08/llama_index/tools/query_engine.py#L13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.query_engine import QueryEngineTool,ToolMetadata\n",
    "\n",
    "sql_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=sql_query_engine,### YOUR CODE HERE\n",
    "    name=\"sql-query\",### YOUR CODE HERE\n",
    "    description=(   \n",
    "      \"Useful for translating a natural language query into a SQL query over a table containing: \"\n",
    "      \"business travel air, containing information wrt company business travel by air \"\n",
    "      \"business travel rail, containing information wrt  company business travel by rail\"\n",
    "        ### YOUR CODE HERE\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Carbon Disclosure Project (CDP) is an international non-profit organization that works with companies, cities, states, and regions to help them disclose their environmental impact. CDP collects and analyzes data on carbon emissions, water usage, and deforestation from thousands of organizations worldwide. This data is then made available to investors, corporations, and policymakers to inform decision-making and drive action towards a more sustainable economy. CDP's mission is to encourage transparency and accountability in environmental reporting, and to promote the adoption of sustainable practices to address climate change and other environmental challenges.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining The Tools Together\n",
    "\n",
    "Now, we can simple add our tools into the `OpenAIAgent`, and off we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_new_agent = OpenAIAgent.from_tools(\n",
    "    tools=[\n",
    "        auto_retrieve_tool,### YOUR CODE HERE\n",
    "        sql_tool### YOUR CODE HERE\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.tools.function_tool.FunctionTool object at 0x2a5bf9890>, <llama_index.tools.query_engine.QueryEngineTool object at 0x2b08a8f50>]\n",
      "ToolMetadata(description='Use this tool to look up semantic information about films.\\nThe vector database schema is given below:\\n{\"metadata_info\": [{\"name\": \"title\", \"type\": \"str\", \"description\": \"title of the emissions reporting methods, one of [Carbon Disclosure Project air, Carbon Disclosure Project rail]\"}], \"content_info\": \"semantic information about carbon disclosure\"}\\n', name='semantic-cdp-info', fn_schema=<class '__main__.AutoRetrieveModel'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.struct_store.sql_query:> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "INFO:llama_index.indices.struct_store.sql_query:> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    }
   ],
   "source": [
    "response = co2_new_agent.chat(\"What is the average CO2 emissions and CH4 emissions for  business travel air . think step by step and show us the steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average CO2 emissions for business travel by air is 431.0, and the average CH4 emissions for business travel by air is approximately 7.15.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.tools.function_tool.FunctionTool object at 0x2a5bf9890>, <llama_index.tools.query_engine.QueryEngineTool object at 0x2b08a8f50>]\n",
      "ToolMetadata(description='Use this tool to look up semantic information about films.\\nThe vector database schema is given below:\\n{\"metadata_info\": [{\"name\": \"title\", \"type\": \"str\", \"description\": \"title of the emissions reporting methods, one of [Carbon Disclosure Project air, Carbon Disclosure Project rail]\"}], \"content_info\": \"semantic information about carbon disclosure\"}\\n', name='semantic-cdp-info', fn_schema=<class '__main__.AutoRetrieveModel'>)\n",
      "INFO:llama_index.indices.struct_store.sql_query:> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "INFO:llama_index.indices.struct_store.sql_query:> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "INFO:llama_index.indices.struct_store.sql_query:> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "INFO:llama_index.indices.struct_store.sql_query:> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "INFO:llama_index.indices.struct_store.sql_query:> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "INFO:llama_index.indices.struct_store.sql_query:> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "> Table desc str: Table 'cdp_business_travel_air' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 3.  Air,,,,,,\n",
      " ID, Description, Length,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n",
      "\n",
      "Table 'cdp_business_travel_rail' has columns: level_0 (TEXT), level_1 (TEXT), level_2 (TEXT), Table 2.  Rail,,,,,,\n",
      " ID, Description,Vehicle ,Miles\" (TEXT), CO2 Emissions (BIGINT), CH4 Emissions (FLOAT), N2O Emissions (FLOAT), and foreign keys: .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    }
   ],
   "source": [
    "response = co2_new_agent.chat(\"What is the average CO2 emissions, CH4 emissions and N2O emissions for Business travel air emissions, also calculate average CO2 emissions, CH4 emissions and N2O emissions for Business travel rail emissions and give me a summary of Co2 ch4 and n2o emissions?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=str(response)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Business travel emissions covered by target (metric tons CO2e)\\nThe average CO2 emissions for business travel by air is \\nThe average CH4 emissions for business travel by air is \\nThe average N2O emissions for business travel by air is \\nThe average CO2 emissions for business travel by rail is \\nThe average CH4 emissions for business travel by rail is \\nThe average N2O emissions for business travel by rail is \\nThe max CO2 emissions for business travel by air is \\nThe max CH4 emissions for business travel by air is \\nThe max N2O emissions for business travel by air is \\nThe max CO2 emissions for business travel by rail is \\nThe max CH4 emissions for business travel by rail is \\nThe max N2O emissions for business travel by rail is \\nIn summary, \\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_travel_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'given this information None and given this template : Business travel emissions covered by target (metric tons CO2e)\\nThe average CO2 emissions for business travel by air is \\nThe average CH4 emissions for business travel by air is \\nThe average N2O emissions for business travel by air is \\nThe average CO2 emissions for business travel by rail is \\nThe average CH4 emissions for business travel by rail is \\nThe average N2O emissions for business travel by rail is \\nThe max CO2 emissions for business travel by air is \\nThe max CH4 emissions for business travel by air is \\nThe max N2O emissions for business travel by air is \\nThe max CO2 emissions for business travel by rail is \\nThe max CH4 emissions for business travel by rail is \\nThe max N2O emissions for business travel by rail is \\nIn summary, \\n and now,please generate a report'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatmsg = \"given this information \"+ res +\" and given this template : \"+business_travel_documents[0]+ \" and now,please generate a report\"\n",
    "chatmsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.tools.function_tool.FunctionTool object at 0x2a5bf9890>, <llama_index.tools.query_engine.QueryEngineTool object at 0x2b08a8f50>]\n",
      "ToolMetadata(description='Use this tool to look up semantic information about films.\\nThe vector database schema is given below:\\n{\"metadata_info\": [{\"name\": \"title\", \"type\": \"str\", \"description\": \"title of the emissions reporting methods, one of [Carbon Disclosure Project air, Carbon Disclosure Project rail]\"}], \"content_info\": \"semantic information about carbon disclosure\"}\\n', name='semantic-cdp-info', fn_schema=<class '__main__.AutoRetrieveModel'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    }
   ],
   "source": [
    "# report = co2_new_agent.chat(\" create a report using {res}\".format(res=response))\n",
    "report = co2_new_agent.chat(chatmsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Travel Emissions Report:\n",
      "\n",
      "Summary of CO2, CH4, and N2O emissions for business travel:\n",
      "\n",
      "- Business travel by air:\n",
      "  - Average CO2 emissions: [Insert average CO2 emissions for business travel by air] metric tons CO2e\n",
      "  - Average CH4 emissions: [Insert average CH4 emissions for business travel by air] metric tons CO2e\n",
      "  - Average N2O emissions: [Insert average N2O emissions for business travel by air] metric tons CO2e\n",
      "  - Maximum CO2 emissions: [Insert maximum CO2 emissions for business travel by air] metric tons CO2e\n",
      "  - Maximum CH4 emissions: [Insert maximum CH4 emissions for business travel by air] metric tons CO2e\n",
      "  - Maximum N2O emissions: [Insert maximum N2O emissions for business travel by air] metric tons CO2e\n",
      "\n",
      "- Business travel by rail:\n",
      "  - Average CO2 emissions: [Insert average CO2 emissions for business travel by rail] metric tons CO2e\n",
      "  - Average CH4 emissions: [Insert average CH4 emissions for business travel by rail] metric tons CO2e\n",
      "  - Average N2O emissions: [Insert average N2O emissions for business travel by rail] metric tons CO2e\n",
      "  - Maximum CO2 emissions: [Insert maximum CO2 emissions for business travel by rail] metric tons CO2e\n",
      "  - Maximum CH4 emissions: [Insert maximum CH4 emissions for business travel by rail] metric tons CO2e\n",
      "  - Maximum N2O emissions: [Insert maximum N2O emissions for business travel by rail] metric tons CO2e\n",
      "\n",
      "In summary, the average CO2 emissions for business travel by air is [Insert average CO2 emissions for business travel by air] metric tons CO2e, while the average CO2 emissions for business travel by rail is [Insert average CO2 emissions for business travel by rail] metric tons CO2e. The average CH4 emissions for business travel by air is [Insert average CH4 emissions for business travel by air] metric tons CO2e, while the average CH4 emissions for business travel by rail is [Insert average CH4 emissions for business travel by rail] metric tons CO2e. The average N2O emissions for business travel by air is [Insert average N2O emissions for business travel by air] metric tons CO2e, while the average N2O emissions for business travel by rail is [Insert average N2O emissions for business travel by rail] metric tons CO2e.\n",
      "\n",
      "The maximum CO2 emissions for business travel by air is [Insert maximum CO2 emissions for business travel by air] metric tons CO2e, while the maximum CO2 emissions for business travel by rail is [Insert maximum CO2 emissions for business travel by rail] metric tons CO2e. The maximum CH4 emissions for business travel by air is [Insert maximum CH4 emissions for business travel by air] metric tons CO2e, while the maximum CH4 emissions for business travel by rail is [Insert maximum CH4 emissions for business travel by rail] metric tons CO2e. The maximum N2O emissions for business travel by air is [Insert maximum N2O emissions for business travel by air] metric tons CO2e, while the maximum N2O emissions for business travel by rail is [Insert maximum N2O emissions for business travel by rail] metric tons CO2e.\n"
     ]
    }
   ],
   "source": [
    "print(str(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    }
   ],
   "source": [
    "wandb_callback.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
